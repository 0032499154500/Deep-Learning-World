.. image:: _img/mainpage/logo.gif

###################################################
Deep Learning for Natural Language Processing
###################################################

.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat
    :target: https://github.com/astorfi/Deep-Learning-NLP/pulls
.. image:: https://badges.frapsoft.com/os/v2/open-source.png?v=103
    :target: https://github.com/ellerbrock/open-source-badge/
.. image:: https://img.shields.io/badge/Made%20with-Python-1f425f.svg
      :target: https://www.python.org/
.. image:: https://img.shields.io/pypi/l/ansicolortags.svg
      :target: https://github.com/astorfi/Deep-Learning-NLP/blob/master/LICENSE
.. image:: https://img.shields.io/github/contributors/Naereen/StrapDown.js.svg
      :target: https://github.com/astorfi/Deep-Learning-NLP/graphs/contributors



##################
Table of Contents
##################
.. contents::
  :local:
  :depth: 4

***************
Introduction
***************

The purpose of this project is to introduce a shortcut to developers and researcher
for finding useful resources about Deep Learning for Natural Language Processing.

============
Motivation
============

There are different motivations for this open source project.

.. --------------------
.. Why Deep Learning?
.. --------------------

------------------------------------------------------------
What's the point of this open source project?
------------------------------------------------------------

There other similar repositories similar to this repository and are very
comprehensive and useful and to be honest they made me ponder if there is
a necessity for this repository!

**The point of this repository is that the resources are being targeted**. The organization
of the resources is such that the user can easily find the things he/she is looking for.
We divided the resources to a large number of categories that in the beginning one may
have a headache!!! However, if someone knows what is being located, it is very easy to find the most related resources.
Even if someone doesn't know what to look for, in the beginning, the general resources have
been provided.


.. ================================================
.. How to make the most of this effort
.. ================================================

************
Papers
************

.. image:: _img/mainpage/article.jpeg

This chapter is associated with the papers published in NLP using deep learning.

====================
Models
====================

-----------------------
Convolutional Networks
-----------------------

.. For continuous lines, the lines must be start from the same locations.
* **Imagenet classification with deep convolutional neural networks** :
  [`Paper <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Convolutional Neural Networks for Sentence Classification** :
  [`Paper <https://arxiv.org/abs/1408.5882>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Large-scale Video Classification with Convolutional Neural Networks** :
  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks** :
  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html>`_]

  .. image:: _img/mainpage/progress-overall-100.png


* **Deep convolutional neural networks for LVCSR** :
  [`Paper <https://ieeexplore.ieee.org/abstract/document/6639347/&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&ei=KknXWYbGFMbFjwSsyICADQ&scisig=AAGBfm2F0Zlu0ciUwadzshNNm80IQQhuhA>`_]

  .. image:: _img/mainpage/progress-overall-60.png

* **Face recognition: a convolutional neural-network approach** :
  [`Paper <https://ieeexplore.ieee.org/abstract/document/554195/>`_]

  .. image:: _img/mainpage/progress-overall-100.png



-----------------------
Recurrent Networks
-----------------------

.. For continuous lines, the lines must be start from the same locations.
* **An empirical exploration of recurrent network architectures** :
  [`Paper <http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **LSTM: A search space odyssey** :
  [`Paper <https://ieeexplore.ieee.org/abstract/document/7508408/>`_]

  .. image:: _img/mainpage/progress-overall-80.png


* **On the difficulty of training recurrent neural networks** :
  [`Paper <http://proceedings.mlr.press/v28/pascanu13.pdf>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Learning to forget: Continual prediction with LSTM** :
  [`Paper <http://digital-library.theiet.org/content/conferences/10.1049/cp_19991218>`_]

  .. image:: _img/mainpage/progress-overall-100.png

-----------------------
Autoencoders
-----------------------

* **Extracting and composing robust features with denoising autoencoders** :
  [`Paper <https://dl.acm.org/citation.cfm?id=1390294>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion** :
  [`Paper <http://www.jmlr.org/papers/v11/vincent10a.html>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Adversarial Autoencoders** :
  [`Paper <https://arxiv.org/abs/1511.05644>`_]

  .. image:: _img/mainpage/progress-overall-60.png

* **Autoencoders, Unsupervised Learning, and Deep Architectures** :
  [`Paper <http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Reducing the Dimensionality of Data with Neural Networks** :
  [`Paper <http://science.sciencemag.org/content/313/5786/504>`_]

  .. image:: _img/mainpage/progress-overall-100.png


-----------------------
Generative Models
-----------------------

* **Exploiting generative models discriminative classifiers** :
  [`Paper <http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Semi-supervised Learning with Deep Generative Models** :
  [`Paper <http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models>`_]

  .. image:: _img/mainpage/progress-overall-80.png


* **Generative Adversarial Nets** :
  [`Paper <http://papers.nips.cc/paper/5423-generative-adversarial-nets>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Generalized Denoising Auto-Encoders as Generative Models** :
  [`Paper <http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models>`_]

  .. image:: _img/mainpage/progress-overall-100.png


-----------------------
Probabilistic Models
-----------------------

* **Stochastic Backpropagation and Approximate Inference in Deep Generative Models** :
  [`Paper <https://arxiv.org/abs/1401.4082>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Probabilistic models of cognition: exploring representations and inductive biases** :
  [`Paper <https://www.sciencedirect.com/science/article/pii/S1364661310001129>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **On deep generative models with applications to recognition** :
  [`Paper <https://ieeexplore.ieee.org/abstract/document/5995710/>`_]

  .. image:: _img/mainpage/progress-overall-100.png





====================
Core
====================

---------------------
Optimization
---------------------

.. ################################################################################
.. For continuous lines, the lines must be start from the same locations.
* **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift** :
  [`Paper <https://arxiv.org/abs/1502.03167>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Dropout: A Simple Way to Prevent Neural Networks from Overfitting** :
  [`Paper <http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Training Very Deep Networks** :
  [`Paper <http://papers.nips.cc/paper/5850-training-very-deep-networks>`_]

  .. image:: _img/mainpage/progress-overall-80.png

* **Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification** :
  [`Paper <https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf>`_]

  .. image:: _img/mainpage/progress-overall-100.png

* **Large Scale Distributed Deep Networks** :
  [`Paper <http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks>`_]

  .. image:: _img/mainpage/progress-overall-100.png


  .. @article{mikolov2013efficient,
  ..   title={Efficient estimation of word representations in vector space},
  ..   author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  ..   journal={arXiv preprint arXiv:1301.3781},
  ..   year={2013}
  .. }

.. ################################################################################

------------------------
Representation Learning
------------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png


---------------------
Transfer Learning
---------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png

-----------------------
Reinforcement Learning
-----------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png


====================
Applications
====================

--------------------
Image Recognition
--------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png

----------------------------
Natural Language Processing
----------------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png


----------------------------
Speech
----------------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png

----------------------------
Caption Generation
----------------------------

* **Efficient Estimation of Word Representations in Vector Space** :
  Two novel model architectures for computing continuous vector representations of words.
  [`Paper link <https://arxiv.org/abs/1301.3781>`_ ,
  `Official code implementation <https://code.google.com/archive/p/word2vec/>`_]

  .. image:: _img/mainpage/progress-overall-100.png






************
Courses
************

.. image:: _img/mainpage/online.png

* **Natural Language Processing with Deep Learning** by Stanford :
  [`Link <http://web.stanford.edu/class/cs224n/>`_]

* **Deep Natural Language Processing** by the University of Oxford:
  [`Link <https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/>`_]

* **Natural Language Processing with Deep Learning in Python** by Udemy:
  [`Link <https://www.udemy.com/natural-language-processing-with-deep-learning-in-python/?siteID=QhjctqYUCD0-KJsvUG2M8PW2kOmJ0nwFPQ&LSNPUBID=QhjctqYUCD0>`_]

* **Natural Language Processing with Deep Learning** by Coursera:
  [`Link <https://www.coursera.org/learn/language-processing>`_]


************
Books
************

.. image:: _img/mainpage/books.jpg

* **Speech and Language Processing** by Dan Jurafsky and James H. Martin at stanford:
  [`Link <https://web.stanford.edu/~jurafsky/slp3/>`_]

* **Neural Network Methods for Natural Language Processing** by Yoav Goldberg:
  [`Link <https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037>`_]

* **Deep Learning with Text: Natural Language Processing (Almost) from Scratch with Python and spaCy** by Patrick Harrison, Matthew Honnibal:
  [`Link <https://www.amazon.com/Deep-Learning-Text-Approach-Processing/dp/1491984414>`_]

* **Natural Language Processing with Python** by Steven Bird, Ewan Klein, and Edward Loper:
  [`Link <http://www.nltk.org/book/>`_]


************
Blogs
************

.. image:: _img/mainpage/Blogger_icon.png

* **Understanding Convolutional Neural Networks for NLP** by Denny Britz:
  [`Link <http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/>`_]

* **Deep Learning, NLP, and Representations** by Matthew Honnibal:
  [`Link <http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/>`_]

* **Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models** by Sebastian Ruder:
  [`Link <https://explosion.ai/blog/deep-learning-formula-nlp>`_]

* **Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models** by Sebastian Ruder:
  [`Link <https://explosion.ai/blog/deep-learning-formula-nlp>`_]

* **Natural Language Processing** by Sebastian Ruder:
  [`Link <http://blog.aylien.com/12-of-the-best-free-natural-language-processing-and-machine-learning-educational-resources/>`_]

* **Probably Approximately a Scientific Blog** by Vered Schwartz:
  [`Link <http://veredshwartz.blogspot.com/>`_]

* **NLP news** by Sebastian Ruder:
  [`Link <http://newsletter.ruder.io/>`_]

* **Deep Learning for Natural Language Processing (NLP): Advancements & Trends**:
  [`Link <https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/>`_]

* **Neural Language Modeling From Scratch**:
  [`Link <http://ofir.io/Neural-Language-Modeling-From-Scratch/?a=1>`_]


************
Tutorials
************

.. image:: _img/mainpage/tutorial.png

* **Understanding Natural Language with Deep Neural Networks Using Torch** by NVIDIA:
  [`Link <https://devblogs.nvidia.com/understanding-natural-language-deep-neural-networks-using-torch/>`_]

* **Deep Learning for NLP with Pytorch** by Pytorch:
  [`Link <https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html>`_]

* **Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks** by Jon Krohn:
  [`Link <https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f>`_]


************
Datasets
************

=====================
General
=====================

* **1 Billion Word Language Model Benchmark**: The purpose of the project is to make available a standard training and test setup for language modeling experiments:
  [`Link <http://www.statmt.org/lm-benchmark/>`_]

* **Common Crawl**: The Common Crawl corpus contains petabytes of data collected over the last 7 years. It contains raw web page data, extracted metadata and text extractions:
  [`Link <http://commoncrawl.org/the-data/get-started/>`_]

* **Yelp Open Dataset**: A subset of Yelp's businesses, reviews, and user data for use in personal, educational, and academic purposes:
  [`Link <https://www.yelp.com/dataset>`_]


=====================
Text classification
=====================

* **20 newsgroups** The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups:
  [`Link <http://qwone.com/~jason/20Newsgroups/>`_]

* **Broadcast News** The 1996 Broadcast News Speech Corpus contains a total of 104 hours of broadcasts from ABC, CNN and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts:
  [`Link <https://catalog.ldc.upenn.edu/LDC97S44>`_]

* **The wikitext long term dependency language modeling dataset**: A collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. :
  [`Link <https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset>`_]

=======================
Question Answering
=======================

* **Question Answering Corpus** by Deep Mind and Oxford which is two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites.
  [`Link <https://github.com/deepmind/rc-data>`_]

* **Stanford Question Answering Dataset (SQuAD)** consisting of questions posed by crowdworkers on a set of Wikipedia articles:
  [`Link <https://rajpurkar.github.io/SQuAD-explorer/>`_]

* **Amazon question/answer data** contains Question and Answer data from Amazon, totaling around 1.4 million answered questions:
  [`Link <http://jmcauley.ucsd.edu/data/amazon/qa/>`_]



=====================
Sentiment Analysis
=====================

* **Multi-Domain Sentiment Dataset** TThe Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains):
  [`Link <http://www.cs.jhu.edu/~mdredze/datasets/sentiment/>`_]

* **Stanford Sentiment Treebank Dataset** The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language:
  [`Link <https://nlp.stanford.edu/sentiment/>`_]

* **Large Movie Review Dataset**: This is a dataset for binary sentiment classification:
  [`Link <http://ai.stanford.edu/~amaas/data/sentiment/>`_]


=====================
Machine Translation
=====================

* **Aligned Hansards of the 36th Parliament of Canada** dataset contains 1.3 million pairs of aligned text chunks:
  [`Link <https://www.isi.edu/natural-language/download/hansard/>`_]

* **Europarl: A Parallel Corpus for Statistical Machine Translation** dataset extracted from the proceedings of the European Parliament:
  [`Link <http://www.statmt.org/europarl/>`_]


=====================
Summarization
=====================

* **Legal Case Reports Data Set** as a textual corpus of 4000 legal cases for automatic summarization and citation analysis.:
  [`Link <https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports>`_]

************
Contributing
************


*For typos, unless significant changes, please do not create a pull request. Instead, declare them in issues or email the repository owner*. Please note we have a code of conduct, please follow it in all your interactions with the project.

========================
Pull Request Process
========================

Please consider the following criterions in order to help us in a better way:

1. The pull request is mainly expected to be a link suggestion.
2. Please make sure your suggested resources are not obsolete or broken.
3. Ensure any install or build dependencies are removed before the end of the layer when doing a
   build and creating a pull request.
4. Add comments with details of changes to the interface, this includes new environment
   variables, exposed ports, useful file locations and container parameters.
5. You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you
   do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.

========================
Final Note
========================

We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.
For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate
your kind feedback and support.
